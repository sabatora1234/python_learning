
画像を選択して白黒画像カラー化を実行します

----------------------------------------------

◆事前準備


①以下のサイトでicbinpICantBelieveIts_seco.safetensorsモデルをダウンロードします。
  (1.99GBあるので時間がかかります)
  https://civitai.com/models/28059

②Google Driveにダウンロードしたモデルをアップロードします。
  (とりあえずはGoogle Driveのルートに)

③Google Colabで「ランタイムの設定」から設定をCPU→GPUに変更してください。

④Google ColabでGoogle Driveのフォルダをマウントします。
  以下をセルに貼り付けて実行します。

from google.colab import drive
drive.mount('/content/drive')

  モデルは別のものを使ってもよいです。
  変える場合は↓の75行目のモデル名(icbinpICantBelieveIts_seco.safetensors)を差し替えてください。


◆実行方法

①以下の「----------------------------------------------」で区切られた３つの部分を
  セルに貼り付けてそれぞれ順番に実行します。

②１番目の部分はある程度実行すると「ファイル選択」ボタンが出るので
  それを押して変換する白黒画像を選択します。

③残りの部分を順番に実行します。


----------------------------------------------

!pip install -q diffusers==0.20.0 transformers xformers git+https://github.com/huggingface/accelerate.git
!pip install -q opencv-contrib-python
!pip install -q controlnet_aux
!pip install omegaconf

# import
from google.colab import files
from diffusers import StableDiffusionControlNetPipeline
from diffusers.utils import load_image

# ファイルアップロード
uploaded_file = files.upload()
input_path = next(iter(uploaded_file))

# 画像読み込み
image = load_image(input_path)
image

----------------------------------------------

import cv2
from PIL import Image
import numpy as np

from diffusers import KDPM2DiscreteScheduler

model_PATH = "ioclab/control_v1p_sd15_brightness"

import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

# 白黒画像カラー化ControlNetモデルと使用する拡散モデルをロード
controlnet = ControlNetModel.from_pretrained(model_PATH, torch_dtype=torch.float16)
pipe = StableDiffusionControlNetPipeline.from_single_file(
    "/content/drive/MyDrive/icbinpICantBelieveIts_seco.safetensors", controlnet=controlnet, torch_dtype=torch.float16
)

# Kerras D2を使用した
pipe.scheduler = KDPM2DiscreteScheduler.from_config(pipe.scheduler.config)

# パイプラインを直接GPUにロードする代わりに、enable_model_cpu_offload関数でCPUオフロードを有効にする
pipe.enable_model_cpu_offload()

# FlashAttention/xformersのアテンションレイヤーアクセラレーションを有効にする
pipe.enable_xformers_memory_efficient_attention()

----------------------------------------------

prompt = ", (((Canon EOS 5D Mark4))),masterpiece, best quality, extremely detailed"
generator = torch.Generator(device="cpu").manual_seed(113)

output = pipe(
    prompt,
    image,
    negative_prompt="monochrome, lowres, bad anatomy, worst quality, low quality",
    guidance_scale=3.5,
    generator=generator,
    num_inference_steps=40,
)

output.images[0]

----------------------------------------------

